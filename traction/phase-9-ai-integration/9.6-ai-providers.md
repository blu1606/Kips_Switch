# 9.6 AI Providers Strategy

> **Goal:** 100% FREE AI v·ªõi latency <300ms  
> **Updated:** 2025-12-10

---

## üéØ User Psychology

**Pain:** "AI features = expensive, slow"  
**Solution:** Multi-provider fallback, all FREE

---

## ‚ö° Provider Priority

| Priority | Provider | Why | Latency |
|----------|----------|-----|---------|
| **Primary** | Groq | Fastest, 14K/day FREE | ~150ms |
| **Fallback 1** | Cerebras | Fast, 14K/day FREE | ~250ms |
| **Fallback 2** | Gemini | Generous, 1M tokens/min | ~400ms |

---

## üìê Architecture

```
Request ‚Üí Cache Check (9.7)
              ‚Üì miss
         Try Groq
              ‚Üì 429 rate limit
         Try Cerebras
              ‚Üì 429 rate limit
         Try Gemini
              ‚Üì
         Store in Cache
```

---

## üîê Safety Layer (Pre-AI Scrubbing)

**Risk:** User accidentally pastes private key into prompt  
**Solution:** Client-side crypto pattern scrubbing BEFORE sending to API

### Scrub Patterns (Reuse Anti-Doxxer regex)
- Ethereum keys: `0x[a-fA-F0-9]{64}`
- Solana keys: `[1-9A-HJ-NP-Za-km-z]{32,44}`
- Seed phrases: 12/24 consecutive dictionary words
- Bitcoin WIF: `[5KL][1-9A-HJ-NP-Za-km-z]{50,51}`

### DO NOT Scrub
- Names, dates, pet names ‚Üí **Needed for context**
- Generic numbers ‚Üí False positives

### Implementation
```
User input ‚Üí scrubCryptoPatterns(input) ‚Üí API call
```
- Replace matches with `[REDACTED_KEY]`
- Log warning to user: "Sensitive data removed for your safety"

### Disclaimer (Show in UI)
> "AI processing happens securely via Groq. No private keys are stored."

---

## üîß Implementation Guide

### Step 1: Environment Setup
```bash
# .env.local
GROQ_API_KEY=gsk_xxx        # console.groq.com/keys
CEREBRAS_API_KEY=csk-xxx    # cloud.cerebras.ai
GOOGLE_AI_API_KEY=AIza...   # aistudio.google.com/apikey
```

### Step 2: Provider Module
- Create `lib/ai/groq.ts`, `cerebras.ts`, `gemini.ts`
- Each exports `generateWithX(prompt): Promise<string>`
- Use `groq-sdk`, `@ai-sdk/google`, etc.

### Step 3: Fallback Router
- Create `lib/ai/provider.ts`
- Try providers in order
- Catch 429 ‚Üí next provider
- Log which provider succeeded

### Step 4: Edge Runtime
- All API routes: `export const runtime = 'edge'`
- Reduces cold start: 1000ms ‚Üí 50ms

### Step 5: Streaming
- Use `streamText()` from Vercel AI SDK
- First token in ~30ms vs waiting 300ms

---

## üìä Capacity Planning

| Scale | Daily Requests | Coverage |
|-------|----------------|----------|
| 100 users | ~500 | ‚úÖ Groq alone |
| 1,000 users | ~5,000 | ‚úÖ Groq + Cerebras |
| 10,000 users | ~50,000 | ‚ö†Ô∏è Need paid tier |

---

## ‚úÖ Execution Checklist

- [ ] Create Groq account (no CC needed)
- [ ] Create Cerebras account (no CC needed)  
- [ ] Create Google AI Studio key
- [ ] Install: `npm i groq-sdk ai @ai-sdk/groq @ai-sdk/google`
- [ ] Create provider modules
- [ ] Create fallback router
- [ ] Test with `/api/ai/test` endpoint
