# 9.7 AI Response Caching

> **Goal:** Cache AI responses Ä‘á»ƒ giáº£m latency vÃ  cost  
> **Storage:** Supabase PostgreSQL  
> **Updated:** 2025-12-10

---

## ðŸŽ¯ User Psychology

**Pain:** "AI is slow, inconsistent"  
**Solution:** Same question = instant response from cache

---

## ðŸ“ Architecture

```
Request â†’ Hash(prompt)
              â†“
         Check Supabase cache
              â†“ hit
         Return cached (30ms)
              â†“ miss
         Call AI Provider (~150ms)
              â†“
         Store async (don't block)
              â†“
         Return response
```

---

## ðŸ—„ï¸ Database Schema

```sql
CREATE TABLE ai_cache (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  prompt_hash TEXT UNIQUE NOT NULL,
  prompt_type TEXT NOT NULL,  -- 'password_hint' | 'write_assist'
  response TEXT NOT NULL,
  model TEXT NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  expires_at TIMESTAMPTZ DEFAULT (NOW() + INTERVAL '24 hours')
);

CREATE INDEX idx_ai_cache_hash ON ai_cache(prompt_hash);
ALTER TABLE ai_cache ENABLE ROW LEVEL SECURITY;
```

> **Note:** Run migration via Supabase MCP server when implementing.

---

## ðŸ”§ Implementation Guide

### Step 1: Cache Service
- Create `lib/ai/cache.ts`
- `getCachedResponse(prompt)` â†’ query by hash
- `setCachedResponse(prompt, response, type)` â†’ upsert
- Hash = SHA256 of normalized prompt

### Step 2: Integrate with Provider
- Check cache BEFORE calling AI
- Store response AFTER success (async)
- Don't block response for cache write

### Step 3: Cache Key Strategy
```
hash = SHA256(prompt.trim().toLowerCase())
```
- Normalize: trim + lowercase
- TTL: 24 hours
- Same semantic question = same hash = cache hit

### Step 4: Cleanup
- Option A: Supabase pg_cron (Pro plan)
- Option B: API route `/api/cron/cleanup-cache`
- Delete where `expires_at < NOW()`

---

## ðŸ“Š Expected Performance

| Metric | Without Cache | With Cache |
|--------|---------------|------------|
| Latency (hit) | N/A | **30ms** |
| Latency (miss) | 150ms | 150ms |
| Hit rate target | 0% | **>60%** |
| API calls saved | 0% | **60%+** |

---

## ðŸŽ¯ Cache Scenarios

| Scenario | Cache Hit? |
|----------|------------|
| Same user, same prompt | âœ… Yes |
| Different user, same prompt | âœ… Yes |
| Similar prompt (different words) | âŒ No |
| Prompt after 24h | âŒ No (expired) |

---

## âœ… Execution Checklist

- [ ] Run migration via MCP: `mcp_supabase-mcp-server_apply_migration`
- [ ] Create `lib/ai/cache.ts`
- [ ] Add `SUPABASE_SERVICE_ROLE_KEY` to env
- [ ] Integrate cache into `lib/ai/provider.ts`
- [ ] Add cache hit/miss logging
- [ ] Create cleanup cron (optional)

---

## ðŸš€ Future: Semantic Caching (v2)

**Current limitation:** "help me write" vs "help write" = cache miss

**v2 Solution:** Supabase pgvector for semantic similarity

```
v1 (Now):  SHA256(prompt) â†’ exact match only
v2 (Later): Embedding(prompt) â†’ cosine similarity > 0.95
```

**Trade-offs:**
| Approach | Latency | Hit Rate | Complexity |
|----------|---------|----------|------------|
| SHA256 | <1ms | ~50% | â­ Easy |
| pgvector | +100ms | ~80% | â­â­â­ Hard |

**Decision:** SHA256 for hackathon. pgvector for production v2.

---

## ðŸŽ¤ Judge Talking Points

> **Q: "What's your cache hit rate?"**  
> A: "We expect 50-60% with exact-match hashing. For v2, we'll implement semantic caching with pgvector to reach 80%+."

> **Q: "Why not use Redis?"**  
> A: "Supabase is already in our stack. One less service = simpler ops. Latency difference (~20ms) is acceptable for MVP."
